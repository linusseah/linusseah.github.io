---
layout: post
title: "Inside Out: Machine Learning & Lexical Rule Based Emotion Classifier for Textual Input"
date: 2020-01-20
excerpt: "Capstone project for General Assembly - Data Science Immersive 11 (Oct 29 2019 - Jan 31 2020). Machine Learning & Lexical Rule Based Emotion Classifier for Text"
project: true
tags: [nlp, recommender system, web scraping, logistic regresion, decision trees, flask]
comments: true
---
## Background  
> "What is on your mind?... What are you thinking about?... A penny for your thoughts?" For most of us, experiencing, intepreting, understanding and reacting to our own emotional states and those around us are very much a part of our daily interactions at the workplace, at home with our family members and other social settings; reacting appropriately to different situations based on the emotional context is key to developing meaningful personal / business relationships. I suspect this is why friends often prefer meeting up in person / business executives insist on having face to face negotiations etc. However, appreciation of emotional context is often done in-person - and for good reason - we read emotions best based on tonality, body-language, facial expressions and many other verbal and non-verbal cues perceived both consciously and sub-consciously. For this reason, emotional interpretation is largely limited to a localised / small settings. 
> But what if we could glean information on emotional responses 'en-masse' using other less conventional methods? For example, by analyzing the 'tonality' based on a particular style of writing or 'expressions' based on punctuations or other key words used from textual input? What if we could tell how a hundred, or a thousand people are feeling at a particular point in time and specifically about what -- in real time no less. These questions motivated my research into the field of Natural Language Processing (NLP) and the development of a tool that is built on both traditional Machine Learning/NLP concepts but also lexical rule-based libraries that other researchers have built to efficiently provide 'emotional data' en-masse starting with text as input.  

## Problem Statement 
It is difficult to gather emotional feedback at scale. A potential way to do so is to mine and interpret readily available but hard to interpret unstructured forms of input (like text on social media). 

The following are some examples of potential painpoints/use cases for 'emotional-listening': 

1. Costly and time-consuming to gather mass implicit feedback: 
Information on the emotional response to an event (could be a press release, a public speech etc) could be helpful for better decision making (e.g. reactions toward a certain action or policy) but obtaining this implicit feedback in real-time and on a mass level is often costly and time-consuming, if even possible. While explicit emotional feedback could be obtained from social media platforms like facebook (e.g by calculating the % of likes, or angry emojis reacts etc) this would only provide a snapshot in reaction to something (i.e. a secondary reaction). It would be more powerful if we could get that same analysis implicitly based on individual's textual input (a primary reaction) on publicly accessible platforms. 

2. Preventive action: 
Unfortunate events like mass-killings are often preceded by telling messages indicative of troubled emotions before the incidents happen but are usually only discovered only post-mortem.    

3. Augmented sentiment analysis: 
Current sentiment analysis is usually binary. However, even within the broad categories of ‘good’ or ‘bad’ it would be helpful to understand what kind of ‘good’ or ‘bad’ -- and at what intensities -- in order to respond more appropriately to the reaction. For example, a product review of an Iphone could have been assigned a 'bad' / negative sentiment score but this negative sentiment could have been disappointment, anger or any other 'negative' emotion for that matter - in which case, (Apple's) response would be very different; the user could be disappointed that a particular feature was not released, or angry that the quality of the phone is sub-par. The former reaction could be considered as a subjective opinion that would be good to keep in mind as a potential feature add on for future releases whereas the latter reaction would need to be re-examined with much greater urgency as the potential ramifications of a faulty phone is much greater.   

## Proposed Solution 
Developing a web-application that takes in textual input and provides real-time predicion of emotions and associated key-words at a sentence level    

## Other Use-cases 
    
1. Emotional analysis as an additional data point for digital marketing 

2. Better recommendation systems (think Spotify songs that are mood appropriate) 

3. Real time 'emotional-listening' used to pre-empt bad events from happening  

4. Used for more granular sentiment analysis (for policy makers, anything thats find an emotional response informative)

5. Mental health tracker: individuals can journal and put their journal through this analyser, which will help to track their emotions at chosen time intervals over time 

6. To help with disabilities: if a speech to text function can pass info through this model to return feedback on emotions, it can help those with an impaired ability to pick up emotional nuances through daily text/speech based interactions.

8. In general, as an additional data point — layered on top of other structured and unstructured data inputs (ppl posting live videos on facebook, insta-stories etc) 

## Methods Applied 
    
1. Natural Language Processing: pre-process text (vectorizing, lemmatizing etc), and to break it down in a way that best captures the context and sentiment of the textual input (e.g. n-grams, number of word features)  

2. Classification modelling: train the model against the tagged emotional state to accurately classify emotions according to textual input using an appropriate statistic model  

3. Lexical Rule-based libraries: lexical libraries that have been manually curated to pre-assign sentiment scores to specific key-words, word orderings etc according to a rule-based approach.  

## Data 
1. Pre-tagged Kaggle dataset for emotions based on 450,000 tweets  
2. Pre-tagged Figure-eight dataset (~50,000) text messages 
3. Pre-tagged Figure-eight dataset (~2,500) text inputs 

## Implementation Considerations  

- How should I score emotions? What kind of scale? (Need to find some kind of objective and standardised way to score text input and map that against an emotional scale) 

- How many emotions do I want to classify? (it could officially go up to 27) 

- How would I delineate one emotion from another? (e.g. anger vs annoyance. Should they exist along some kind of spectrum?) 

- how do I deal with complex emotions? (individuals can experience multiple emotions at once) -> would be nice to map it onto a spider graph and show how much of each emotion is being expressed through that textual input 

- what other features are relevant for this study? (e.g. keep layering study with other data inputs like bio-chemical info, personality etc) 

- What is the level of granularity that i should be breaking down the text input to for analysis? (n-gram word level, sentence, paragraph?) 


# Data Cleaning and Pre-Processing

## Combining 3 datasets 

The Kaggle dataset (~415,000 tweets) had been pre-tagged according to 6 emotional states ('sadness', 'joy', 'anger', 'fear', 'surprise'). As I didn't have to scrape for these inputs off Twitter directly there was much less pre-processing to be done (e.g. in terms of cleaning up the HTML or accessing the correct data in a nested dictionary as is usually the case with raw data scrapes)  

The Figure eights datasets had fewer observations (~40,000 and 2,500 respectively) but a wider range of 13 and 18 emotions. Similar to the kaggle dataset, Figure eight had already ordered the inputs nicely into a .csv file which I could load directly into my Jupyter notebook for further processing.   

Overview of the emotions captured in the 3 datasets:

1. 'sadness', 'joy', 'love', 'anger', 'fear', 'surprise'

2. 'empty', 'sadness', 'enthusiasm', 'neutral', 'worry', 'surprise', 'love', 'fun', 'hate', 'happiness', 'boredom', 'relief', 'anger'

3. 'Neutral', 'Anger', 'Optimism', 'Disgust', 'Sadness','Anticipation', 'Aggression', 'Submission', 'Love', 'Surprise','Contempt', 'Disapproval', 'Remorse', 'Ambiguous', 'Fear', 'Joy',

After cleaning out the unecessary columns and re-naming some column headings to ensure consistency, I concatenanted all 3 datasets containing all emotional states.  

<figure>
	<img src="../assets/img/allemo_chart.png">
	<figcaption>Bar chart showing all emotions from 3 datasets (%) </figcaption>
</figure>

## Problem of Imbalanced Data

There was clearly a long-tail of (more nuanced) emotion-types that had far less observations than other 'plain-vanilla' emotional types like 'joy', 'anger' etc. This would lead to an imbalanced dataset that would impact the model's prediction accuracy as it would not have as many datapoints of the less represented emotions to be sufficiently 'trained' on. 

For this reason, I chose to subset only the top 8 emotions by count to form the final dataset that I would be training my model on. 

<figure>
	<img src="../assets/img/emo_short_perc.png">
	<figcaption> % Breakdown of Emotion Shortlist </figcaption>
</figure>


<figure>
	<img src="../assets/img/emo_short.png">
	<figcaption>Bar chart showing Emotion Shortlist </figcaption>
</figure>

However, as we can tell, even after shortlisting the top 8 emotions by count, there is still a rather large imbalance (as % of total observations) between top 8 emotions. I plan to deal with this using traditional random upsampling within each emotion-class after train-test-split, in order to prevent the same observations from being present in both the training and testing sets. 

I did not balance out the classes during the construction of my baseline-model. 

> briefly discuss up-sampling techniques, what SMOTE is and why it might not work in this context 
> discuss the performance difference btw baseline VS tuned model 
> include more classification evaluation metrics (e.g. recall and precision and not just accuracy) 

## (to augment model) 
> upsample either via SMOTE or normal random upsampling 
> increase word feature to 7500
> use TFIDF instead of count vect 
> try to find out if you can keep emoticons (in the text format? instead of stripping it?) 
> remove 'no' or 'nor' and other negation type words to preserve the intention of the sentence (e.g. to prevent 'not happy' from getting stripped to 'happy') 
> add more rigorous classification evaluation metrics 
> try out RNN (maybe) 

## Cleaning up training set text input 

I wrote a function that takes in the raw text input from the training set, strips it of non-letters, converts it to lower case, removes all stopwords and lemmatizes the words before concatenating individual words back into a 'cleaned' string. 

I used this 'cleaned' input as the basis for the building of my base line model. 

{% highlight python %}

import libraries
from bs4 import BeautifulSoup   
import regex as re
from nltk.corpus import stopwords 
from nltk.stem import WordNetLemmatizer

# Instantiate lemmatizer 
lemmatizer = WordNetLemmatizer()

# Cleaning Function 
def clean_text(raw_post):
    
    # 1. Remove HTML.
    review_text = BeautifulSoup(raw_post).get_text()
    
    # 2. Remove non-letters.
    letters_only = re.sub("[^a-zA-Z]", " ", review_text)
    
    # 3. Convert to lower case, split into individual words.
    words = letters_only.lower().split()
    # Notice that we did this in one line!
    
    # 4. In Python, searching a set is much faster than searching
    # a list, so convert the stop words to a set.
    stops = set(stopwords.words('english'))
    
    # 5. Remove stop words.
    meaningful_words = [w for w in words if not w in stops]
    
    # 6. Lematize 
    lem_meaningful_words = [lemmatizer.lemmatize(i) for i in meaningful_words]
    
    # 7. Join the words back into one string separated by space, 
    # and return the result.
    return(" ".join(lem_meaningful_words))

{% endhighlight %}

Looping through raw text inputs to clean the entire training set. 

{% highlight python %}

total_posts = emo_short.shape[0]
print(f'There are {total_posts} total posts.')

# Initialize an empty list to hold the clean posts.
clean_posts = []

print("Cleaning and parsing the whole set...")

j = 0
for i in emo_short['text']:
    # Convert post to words, then append to clean_train_posts.
    clean_posts.append(clean_text(i))
    
    # If the index is divisible by 10000, print a message
    if (j + 1) % 10000 == 0:
        print(f'Post {j + 1} of {total_posts}.')
    
    j += 1

{% endhighlight %}

There were 5519 values that turned up null after the cleaning process. That's 1.22% of the initial total observations - not too bad.

## Cleaning difficulties 

1. Selectively keeping special characters that convey useful information e.g. emojis 
> look at Clarence's Emoji file 

2. Removing/contracting words with character repetitions (e.g. whaaaaaat => what)
> apprently you have to create a manual dictionary that removes specific words (complete fail safe method)
> otherwise you could create a generalizable regex code that contracts anything with more than 2 repeated characters to a single character - though this would leave out words that have only two repeated characters but it would still be an improvement over the raw dataset. 

3. Transforming contracted words to their normal forms e.g. can't to cannot (so that the same word is not counted as two distinct words) 

4. Removing stop words while maintaining negations (e.g. not, no, nor) to preserve the intention.

5. Separating 'concatenated' words (e.g. iloveyoutodeath) 

6. Correcting mis-spelt words (e.g. 'beleive' vs 'believe') 

# Creating Baseline Model

## 1. Train-Test-Split 

First we have to split the data set into a training set which is used to train the model on, and a testing set which is used to test the efficacy of the trained model. 

{% highlight python %}

# Import train_test_split.
from sklearn.model_selection import train_test_split

X = emo_short['clean_text']
y = emo_short['emotions_label']

# Create train_test_split.
X_train, X_test, y_train, y_test = train_test_split(X, #this is the X (predictor vars)
                                                    y, #this is the y (target var)
                                                    #stratify=y, #preserves the ratio of X:y in train and test sets
                                                    test_size = 0.25,
                                                    random_state = 42)
{% endhighlight %}


## 2. Vectorizing word features in training and testing sets 

Next we have to vectorize the word features to convert it from word to numerical format in order for the 'machine learning' to take place. 

There are 2 types of vectorizers that can be used, Countvectorizer counts the number of each unique word feature in a given string while Term-frequency Inverse Document Frequency (TFIDF) Vectorizer assigns a score to each word feature based on the number of times that word is present in a sentence relative to it's presence across all other sentences in a given dataset (or body of text). 

{% highlight python %}
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# Instantiate our Vectorizers.
cvec = CountVectorizer(stop_words = 'english', 
                       max_features = 2000, 
                       min_df = 10)

tvec = TfidfVectorizer(stop_words = 'english', 
                       max_features = 2000, 
                       min_df = 10)

# Fit & transform our Count/TFID Vectorizer on the training data and transform training data.
X_train_cvec = cvec.fit_transform(X_train)
X_train_tvec = tvec.fit_transform(X_train)

X_train_cvec_df = pd.DataFrame(X_train_cvec.toarray(), 
                               columns = cvec.get_feature_names())

X_train_tvec_df = pd.DataFrame(X_train_tvec.toarray(), 
                               columns = tvec.get_feature_names())

#Transform our test set 
#reminder to self: we only transform and not fit the test set again because we only want features present in the 
#training set to be used for the test set (otherwise model trained on training set cannot be used on the test set)
X_test_cvec = cvec.transform(X_test).todense() 
X_test_tvec = tvec.transform(X_test).todense() 

X_test_cvec_df = pd.DataFrame(X_test_cvec, 
                              columns = cvec.get_feature_names())

X_test_tvec_df = pd.DataFrame(X_test_tvec, 
                              columns = tvec.get_feature_names())

{% endhighlight %}

I started off by using the CountVectorizer. The results below are a snippet of the original output after converting word features into numerical form using the CountVectorizer. We are clearly facing a **sparse-matrix problem**. A sparse-matrix occurs when there are many unique (word) features that occur only once or a few times across in the entire dataset, leaving the matrix with many null values, and hence resulting in a 'sparse-matrix'. This is problematic because we now have an extremely tall (high number of rows) and wide (high number of columns) dataset that makes it computationally expensive for the model to train on. 

For context, without restricting the number of word features (as one of the parameters) when using the CountVectorizer on my dataset, the total number of unique features total to **70,000+**. Given that my training set has almost 350,000 rows, the entire matrix would contain a whopping **23,425,430,000** cells -- Yes, that is a whopping 23+ billion values that the computer has to read into memory if we were training on this sparse matrix. In fact, when I tried it my system's kernel crashed -- clearly not a good idea. 

<figure>
	<img src="../assets/img/sparse_m.png">
	<figcaption> Sparse matrix containing unique word features </figcaption>
</figure>

I ended up limiting the number of word features to 2,000 to keep the matrix more manageable (669,298,000 cells - still no small number, but far more manageable). I further restricted the unique word feature set by limiting it to the selection of words that appeared in the dataset at least 10 times - in order to prevent meaningless words from being captured. (e.g. colloquialisms like 'aaaaaaah!!!!' or 'ssssshhhiiitttt') 

## Finding the most frequently used words 

After vectorizing the word features, I combined both training and test sets to find out which words had the highest number of counts for each of the 8 emotions. 

{% highlight python %}
joy_df = train_cvec_combined_df[train_cvec_combined_df['emotions_label']==2]
joy_df.drop('emotions_label', axis=1, inplace = True)
top_20_joy = joy_df.sum().sort_values(ascending=False).head(20)
top_20_joy.plot.barh(figsize=(11,6));
{% endhighlight %}

<figure>
	<img src="../assets/img/top20_joy.png">
	<figcaption> Top 20 word features for 'Joy' </figcaption>
</figure>

The key observation here was that there was quite a few overlapping frequently used words across the 8 emotion classes, most of which were not particularly informative (e.g. words like 'im', 'feeling', 'feel', 'know', 'day'). This indicated that perhaps CountVectorizer might not be the best vectorizer to use in this instance as it was capturing frequently used words that were not meaningful in distinguishing between the different emotional classes. 

This was confirmed when I combined the top 20 words across all 8 emotion classes to find that that only 59 out of 160 or 36.8% of the most frequently used words were unique. 

**There were a few improvements that I can make based on these observations**:
1. Use a TFIDF Vectorizer since it will penalize terms that get used too frequently across the dataset
2. Expand my list of stopwords to include less meaningfull words like word contractions 

## Fitting the Baseline Model 








{% highlight python %}

{% endhighlight %}

{% highlight python %}

{% endhighlight %}

{% highlight python %}

{% endhighlight %}

## Difficulties
    
1. Text as a pure emotion analyser would be difficult as there is so much that is lost when we exclude the verbal intonations, facial expressions etc so can think about how you can compensate for those things in your analysis (e.g. picking up bold, or italics, exclamations, or emojis etc) could also consider how I can maybe augment the dataset with other features to better predict for emotion? 


2. a single textual input could referce multiple emotional states at varying points in time e.g. someone recouting his or her emotions throughout the day. How to reconcile this? the overall emotional state on balance? 


3. things like sarcasm, or passive-aggressiveness would be hard to detect and even so, to classify as a particular emotion


4. pejoratives (words expressed as neutral or even positive statements to mask a negative sentiment) e.g. 'the chicken had a unique taste... or his idea was interesting..." 


5. picking up emotional-nuances expressed in non-textual cues like emoticons or accents like bold, italics etc e.g. ... or :>, **angry** etc 


6. slang is localised. so the model would probably not perform well in different cultural contexts unless it is trained on multiple localised datasets (e.g. American slang used to connote certain emotions are not the same as those that Singaporeans would use) slang is also dynamic in nature (this is why things like urban dictionary exist because new words are always getting added to it) this also means that the model will have to be continuously trained on new datasets/ add new words to an emotion library 


7. lexical methods are context specific; vocab that connotes specific emotion differ from setting to setting. (e.g. lexical anaylsis for social media would be alot more slang/emoticon heavy whereas the same for formal writing would not) So a model trained on a social media based dataset would probably not do as well when interpreting the emotions of formal prose. 


8. humans experience complex emotions (it's hard enough for a human to accurately interpret emotion, not to mention computers...) so it would be important to objectively quantify that complex expression in some way. Also to figure out which emotions are compatible with each other and which aren't. (e.g. you can be both happy and excited at the same time... but happy and angry? ... possible?) "locating your current emotional state in a N-dimensional emotional plane - plotted over time would form your emotional cloud "
note to self: i think this can be captured in the theory of valence - which classifies emotions into generally positive or negative sentiments
https://en.wikipedia.org/wiki/Valence_(psychology)


9. dealing with imbalanced classes of emotions (general emotions like joy or sadness would be classified far more often than more nuanced ones like emptiness..) more broadly speaking, the process of classifying emotions is tricky because sometimes we don't even know how to particularize our emotions in words.  


10. dataset is currently being trained largely on individual sentences. Not sure how well the model would perform on larger chunks of texs e.g. paragraphs - **find some way to split the analysis up to the sentence? e.g. within paragraphs**  


11. will cleaning technique differ depending on subsequent method of analysis adopted? (e.g. if VADER uses emoticons and Caps to detect valence then we probably shouldnt strip the text of non-textual input and lowercase everything)










